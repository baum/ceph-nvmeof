version: "3.8"
services:
  ceph-spdk:
    image: ceph/spdk
    profiles:
      - build
    build:
      context: spdk/
      dockerfile: ../Dockerfile.spdk
      target: ceph-spdk
      args:
        PKGDEP_ARGS: --rbd
        CONFIGURE_ARGS: >-
          --with-rbd
          --disable-tests
          --disable-unit-tests
          --disable-examples
  ceph-cluster:
    image: quay.io/ceph/vstart-cluster
    container_name: ceph-cluster
    build:
      context: .
      dockerfile: Dockerfile.ceph
    environment:
      VSTART_ARGS: >-
        --without-dashboard
        --memstore
      TOUCHFILE: /tmp/ceph-cluster.touch
    entrypoint: |
      sh -c './vstart.sh --new $$VSTART_ARGS &&
      ceph osd pool create rbd &&
      sleep infinity'
    healthcheck:
      test: ceph osd pool stats rbd
      start_period: 5s
      interval: 5s
    volumes:
      - ceph-conf:/etc/ceph
    networks:
      default:
        ipv4_address: 192.168.13.2
  ceph-nvmeof-base:
    image: quay.io/ceph/nvmeof
    build:
      context: .
      target: ceph-nvmeof
    hostname: ceph-nvmeof
    volumes:
      # sudo bash -c 'echo 1024 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages'
      # https://spdk.io/doc/containers.html
      # TODO: Pending of https://github.com/spdk/spdk/issues/2973
      - /dev/hugepages:/dev/hugepages
      - ceph-conf:/etc/ceph:ro
    cap_add:
      - SYS_ADMIN
      - CAP_SYS_NICE
    networks:
      default:
        ipv4_address: 192.168.13.3
    ports:
      - "4420:4420" # I/O controllers
      - "5500:5500" # Gateway
      - "8009:8009" # Discovery
  ceph-nvmeof:
    extends:
      service: ceph-nvmeof-base
    depends_on:
      ceph-cluster:
        condition: service_healthy
  ceph-nvmeof-devel:
    # Runs from source code in current dir
    extends:
      service: ceph-nvmeof-base
    depends_on:
      ceph-cluster:
        condition: service_healthy
    volumes:
      - .:/src
  ceph-nvmeof-cli:
    image: quay.io/ceph/nvmeof-cli
    build:
      context: .
      target: ceph-nvmeof-cli
volumes:
  ceph-conf:
networks:
  default:
    ipam:
      config:
        - subnet: 192.168.13.0/24
